# Dataset Setup

Due to the nature of using multiple datasets, the dataset setup will probably be the hardest part of getting this code to run properly. Here I try my best to provide a streamlined method for setting up the datasets. Please follow the instructions carefully, and note that there is a bit of manual work that needs to be done by the end user if new datasets are to be added. 

## Setting up dataset folders

Here is an example of how I set up my dataset folder. For me this is under the path `/net/acadia14a/data/dokim/mseg_dataset/`:
```bash
    ├── BDD
    │   ├── ...
    ├── Cityscapes                      # Cityscapes folder
    │   ├── gtFine                      # folder with ground truths
    │   ├── leftImg8bit                 # folder with images
    │   ├── cityscapes_val_lists.pkl    # file with validation image and mask paths, + image names
    │   └── cityscapes_train_lists.pkl  # file with train image and mask paths, + image names
    └── ...
```

Here, the files `cityscapes_{split}_lists.pkl` are dictionaries that contain the image and mask paths, as well as the
 file names. I've included an example of this, and you can check its values by firing up a python console in this
  folder and running the following code:
  
```python
import pickle
with open('cityscapes_val_lists.pkl', 'rb') as f:
    data = pickle.load(f)

print(data.keys())
print(data['images'][0])
```
Note that the order of `data['images']` and `data['masks']` are in correspondance with each other. 

**When setting up a new dataset, this file should be created for both the train and validation sets.** For test
 datasets, such as KITTI or WildDash, only the validation file is needed. Also, **make sure the image and mask paths
  contain the absolute path to the file, not the relative path**. 
  
The first step is complete! Now, when you load the dataset object for the first time, it will generate an LMDB
 database on the dataset's root path. This may take a minute or two, but after it's generated for the first time, it
  will just read the existing LMDB database.

### Camvid Dataset
The Camvid dataset provides labels in colored images, rather than single-value GT masks. Thus, we should first map
 the colors to integers (changing a H x W x C image to H x W). I've provided the code to do so in `datasets/camvid.py
 ` under the function `remap_camvid_labels`. You may need to change the root and labels path according to your folder
  structure.

## Dataset GT Mappings
This is probably the most complicated part of the dataset setup. These steps need to be followed in order.

### Define category names and mappings
First, we need to add the category names of each dataset, as well as their mapping values. These mapping values
 correspond to those that would be used to train a model on that specific dataset only. For example, under `datasets
 /dataset_utils.py` in the variable `label_2_train`, I have the mappings for the Cityscapes dataset defined. This
  mapping defines the `label index --> train index`. 
  
In the variable `dataset_categories`, I have the list of category names. **This list of category names is in the same
 order as the mapping defined in `label_2_train`!!** Taking Cityscapes as an example, the mapping value `[7, 0
 ]` means that the label index 7 would be mapped to training index 0, and this corresponds to the `road` class (first
  class name in `dataset_categories['cityscapes']`). Since the unified mapping is generated under the assumption that
   these names are in the correct order, this must be correct. 
   
### Generate unified label space mappings
Once the mappings and category names of each dataset is added, we can generate the unified label space. The unified
 label space is generated by matching names of the categories, **so make sure that any classes need to be merged
  contain the same category name, including symbols, spaces, and upper/lowercase letters**. 
  
I've provided the necessary functions to generate the unified label space. These functions are in the file `datasets
/dataset_utils.py`. Open up a python console, import the following functions, and copy the outputs to the
 `dataset_utils` file. Here is an example, using Cityscapes, IDD, BDD, and Mapillary datasets:
 
 ```python
from dataset_utils import get_dataset_category_union, get_combined_dataset_label2train, get_combined_ind_mapping

dataset_names = ['cityscapes', 'idd', 'bdd', 'mapillary']
category_union = get_dataset_category_union(dataset_names)      # Names of all categories, in alphabetical order
combined_l2t = get_combined_dataset_label2train(dataset_names)  # Label2train values for the unified label space
ind_mappings = get_combined_ind_mapping(dataset_names)          # Mappings for validation.
``` 
In the example above, all functions return a dictionary with keys corresponding to the dataset names. 
* The `category_union` contains the names of all categories, in alphabetical order. This was added to the
  `dataset_categories` variable with the key `city_idd_bdd_mapillary` in my original code.
* `combined_l2t` contains the unified mappings. This was added to the `label2train` variable, under the key format
 `{dataset_name}_to_combined`
* `ind_mappings` contains the mappings for validation. Essentially, this remaps the original dataset mappings such
 that the indexes are in alphabetical order of the category names. This is required because for evaluation, only the
  appropriate classes for each dataset are selected. Since the unified mapping is in alphabetical order, the
   validation mappings must also be in alphabetical order. For example, the `bicycle` class in Cityscapes is
    originally mapped to the last index for Cityscapes training. However, since `bicycle` is the first class from
     Cityscapes in alphabetical order, it is mapped to the 0th index. 
    * The values of `ind_mappings` should be added to the `label2train` variable, under the key format `{dataset_name
    }_to_comb_ind`
    
Make sure to add the outputs to the `dataset_utils.py` file, since these values will be used in various parts of
 training/validation. 
Also, note that for test datasets, we do not need to find the unified mapping. Refer to the provided `dataset_utils
` file.

### New datasets
For new datasets, you can follow the procedure above. The new dataset name must be included in `dataset_names
 = ...` to get the unified label space with the new dataset. Also, if you add a new dataset, you must make sure the
  `num_classes` and `num_datasets` parameters are set to the correct values in `configs/method/{}.yaml`
  
## (Optional) Resizing Datasets
For faster training, I opted to resize the train splits of each training dataset such that the shorter side is 1080
 pixels. This is not mandatory, since the transformation function does this during run-time as well. However, I've
  found that resizing the training set beforehand actually makes training much faster. This is because datasets like
   Mapillary or Cityscapes have very large image sizes, so the image loading time & resizing time is a major
    bottleneck if resizing is done in run-time. Again, this is not necessary, but if it is done, the image path
     should point to the correct resized images before generating the LMDB database. 
     
Also, if you've generated the LMDB database before resizing, and want to use resized images, make sure to delete the
 original LMDB folder and repeat step 1. 

